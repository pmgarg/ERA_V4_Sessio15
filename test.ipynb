{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w8wmqv8ntq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "⚠️  ERROR: No trained model found!\n",
      "======================================================================\n",
      "\n",
      "The checkpoint file 'checkpoint_5500.pt' does not exist.\n",
      "\n",
      "Please run the training cell FIRST to create the model.\n",
      "\n",
      "The training cell will:\n",
      "  1. Create and train DeepSeek-V3 from scratch\n",
      "  2. Save the trained model to 'checkpoint_5500.pt'\n",
      "  3. Then you can run this generation cell\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# TEXT GENERATION - Run this AFTER training completes\n",
    "import torch\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = \"checkpoint_5500.pt\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(\"=\" * 70)\n",
    "    print(\"⚠️  ERROR: No trained model found!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nThe checkpoint file '{checkpoint_path}' does not exist.\")\n",
    "    print(\"\\nPlease run the training cell FIRST to create the model.\")\n",
    "    print(\"\\nThe training cell will:\")\n",
    "    print(\"  1. Create and train DeepSeek-V3 from scratch\")\n",
    "    print(\"  2. Save the trained model to 'checkpoint_5500.pt'\")\n",
    "    print(\"  3. Then you can run this generation cell\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "else:\n",
    "    # Load the trained model\n",
    "    print(\"Loading trained model...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    config = checkpoint['config']\n",
    "    model = DeepSeekV3ForCausalLM(config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\"Using device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\n✅ Model loaded successfully!\")\n",
    "    print(f\"Parameters: {checkpoint['total_params']:,}\")\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Generate text\n",
    "    def generate_text(prompt_text, max_new_tokens=100, temperature=0.8, top_k=50):\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompt_text)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt_text}'\")\n",
    "        print(f\"Generating {max_new_tokens} tokens...\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 70)\n",
    "        return generated_text\n",
    "\n",
    "    # Example generations\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEXT GENERATION EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Example 1\n",
    "    generate_text(\"Once upon a time\", max_new_tokens=50, temperature=0.8)\n",
    "\n",
    "    # Example 2\n",
    "    generate_text(\"The meaning of life is\", max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "    # Example 3 - Custom prompt (uncomment to use)\n",
    "    # generate_text(\"Your custom prompt here\", max_new_tokens=100, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4ck8qupl",
   "metadata": {},
   "source": [
    "# PHASE 1: Train for 5000 steps and save checkpoint\n",
    "\n",
    "Run this cell first. It will:\n",
    "- Train your model for exactly 5000 steps\n",
    "- Automatically stop after 5000 steps\n",
    "- Save a checkpoint to `checkpoint_5000.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pwb574lt2yt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: TRAINING FOR 5000 STEPS (Plain PyTorch)\n",
      "======================================================================\n",
      "\n",
      "[Using FULL DeepSeek-V3-135M architecture]\n",
      "\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "\n",
      "Model Configuration:\n",
      "  - Architecture: DeepSeek-V3 (MLA + MoE)\n",
      "  - Vocabulary size: 50,257\n",
      "  - Hidden size: 576\n",
      "  - Layers: 30\n",
      "  - Attention heads: 9\n",
      "  - KV LoRA Rank: 512\n",
      "  - MoE Experts: 1 Shared + 8 Routed\n",
      "  - Total parameters: 144,152,448\n",
      "\n",
      "Training Configuration (Phase 1):\n",
      "  - Max steps: 5000\n",
      "  - Batch size: 2\n",
      "  - Gradient accumulation: 8\n",
      "  - Effective batch size: 16\n",
      "  - Warmup steps: 100\n",
      "  - Max learning rate: 0.001\n",
      "  - Logging interval: Every 50 steps\n",
      "\n",
      "======================================================================\n",
      "INITIAL LOSS CHECK (before training)\n",
      "======================================================================\n",
      "\n",
      "Initial loss (random weights): 10.9524\n",
      "Expected loss for random model: ~10.82\n",
      "\n",
      "✅ Loss is in expected range for random initialization!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STARTING PHASE 1 TRAINING\n",
      "Will stop at EXACTLY 5000 steps\n",
      "Logging every 50 steps\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/5000 [00:34<47:24:06, 34.14s/step, loss=11.0767, lr=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/5000 | Loss: 11.0767 | LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50/5000 [11:44<16:01:13, 11.65s/step, loss=6.3789, lr=0.000490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50/5000 | Loss: 6.3789 | LR: 0.000490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 100/5000 [22:14<19:10:30, 14.09s/step, loss=6.0371, lr=0.000990]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/5000 | Loss: 6.0371 | LR: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 150/5000 [34:20<19:36:01, 14.55s/step, loss=5.4573, lr=0.000990]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150/5000 | Loss: 5.4573 | LR: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 200/5000 [46:21<18:54:24, 14.18s/step, loss=5.0912, lr=0.000980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200/5000 | Loss: 5.0912 | LR: 0.000980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 250/5000 [57:59<18:19:57, 13.89s/step, loss=4.9628, lr=0.000970]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250/5000 | Loss: 4.9628 | LR: 0.000970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 300/5000 [1:09:31<17:46:52, 13.62s/step, loss=4.5092, lr=0.000959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300/5000 | Loss: 4.5092 | LR: 0.000959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 350/5000 [1:21:04<17:48:06, 13.78s/step, loss=4.1186, lr=0.000949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350/5000 | Loss: 4.1186 | LR: 0.000949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5000 [1:32:22<17:23:53, 13.62s/step, loss=3.8448, lr=0.000939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400/5000 | Loss: 3.8448 | LR: 0.000939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 450/5000 [1:43:34<17:03:31, 13.50s/step, loss=3.3757, lr=0.000929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450/5000 | Loss: 3.3757 | LR: 0.000929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 500/5000 [1:54:40<16:30:51, 13.21s/step, loss=2.7606, lr=0.000919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500/5000 | Loss: 2.7606 | LR: 0.000919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 550/5000 [2:05:41<16:22:39, 13.25s/step, loss=1.8383, lr=0.000908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 550/5000 | Loss: 1.8383 | LR: 0.000908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 600/5000 [2:16:28<16:02:14, 13.12s/step, loss=1.0244, lr=0.000898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600/5000 | Loss: 1.0244 | LR: 0.000898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 650/5000 [2:27:30<15:59:58, 13.24s/step, loss=0.7611, lr=0.000888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 650/5000 | Loss: 0.7611 | LR: 0.000888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 700/5000 [2:38:30<15:50:25, 13.26s/step, loss=0.4687, lr=0.000878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700/5000 | Loss: 0.4687 | LR: 0.000878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 750/5000 [2:49:29<15:30:02, 13.13s/step, loss=0.3708, lr=0.000868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 750/5000 | Loss: 0.3708 | LR: 0.000868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 800/5000 [3:00:25<15:22:05, 13.17s/step, loss=0.3054, lr=0.000857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800/5000 | Loss: 0.3054 | LR: 0.000857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 850/5000 [3:11:09<14:46:56, 12.82s/step, loss=0.3171, lr=0.000847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 850/5000 | Loss: 0.3171 | LR: 0.000847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 900/5000 [3:22:11<15:15:02, 13.39s/step, loss=0.2789, lr=0.000837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900/5000 | Loss: 0.2789 | LR: 0.000837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 950/5000 [3:33:08<14:36:41, 12.99s/step, loss=0.2323, lr=0.000827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 950/5000 | Loss: 0.2323 | LR: 0.000827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1000/5000 [3:43:59<14:32:43, 13.09s/step, loss=0.2449, lr=0.000817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/5000 | Loss: 0.2449 | LR: 0.000817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 1050/5000 [3:54:58<14:18:56, 13.05s/step, loss=0.2143, lr=0.000806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1050/5000 | Loss: 0.2143 | LR: 0.000806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 1100/5000 [4:06:00<14:22:26, 13.27s/step, loss=0.1875, lr=0.000796]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100/5000 | Loss: 0.1875 | LR: 0.000796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1150/5000 [4:16:59<14:14:54, 13.32s/step, loss=0.2021, lr=0.000786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1150/5000 | Loss: 0.2021 | LR: 0.000786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 1200/5000 [4:27:50<13:46:51, 13.06s/step, loss=0.1903, lr=0.000776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200/5000 | Loss: 0.1903 | LR: 0.000776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 1250/5000 [4:38:46<13:27:44, 12.92s/step, loss=0.1815, lr=0.000766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1250/5000 | Loss: 0.1815 | LR: 0.000766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 1300/5000 [4:49:40<13:22:43, 13.02s/step, loss=0.1952, lr=0.000755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300/5000 | Loss: 0.1952 | LR: 0.000755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 1350/5000 [5:00:33<13:17:13, 13.11s/step, loss=0.2038, lr=0.000745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1350/5000 | Loss: 0.2038 | LR: 0.000745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 1400/5000 [5:11:30<13:03:41, 13.06s/step, loss=0.1839, lr=0.000735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400/5000 | Loss: 0.1839 | LR: 0.000735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 1450/5000 [5:22:26<12:51:17, 13.04s/step, loss=0.1529, lr=0.000725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1450/5000 | Loss: 0.1529 | LR: 0.000725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 1500/5000 [5:33:22<12:46:24, 13.14s/step, loss=0.1309, lr=0.000714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500/5000 | Loss: 0.1309 | LR: 0.000714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1550/5000 [5:44:14<12:25:42, 12.97s/step, loss=0.1545, lr=0.000704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1550/5000 | Loss: 0.1545 | LR: 0.000704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 1600/5000 [5:55:11<12:25:16, 13.15s/step, loss=0.1497, lr=0.000694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600/5000 | Loss: 0.1497 | LR: 0.000694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1650/5000 [6:06:09<12:24:56, 13.34s/step, loss=0.1373, lr=0.000684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1650/5000 | Loss: 0.1373 | LR: 0.000684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 1700/5000 [6:17:10<12:10:43, 13.29s/step, loss=0.1363, lr=0.000674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700/5000 | Loss: 0.1363 | LR: 0.000674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 1750/5000 [6:28:04<11:53:43, 13.18s/step, loss=0.1381, lr=0.000663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1750/5000 | Loss: 0.1381 | LR: 0.000663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 1800/5000 [6:39:02<11:37:34, 13.08s/step, loss=0.1227, lr=0.000653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800/5000 | Loss: 0.1227 | LR: 0.000653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 1850/5000 [6:50:01<11:28:10, 13.11s/step, loss=0.1238, lr=0.000643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1850/5000 | Loss: 0.1238 | LR: 0.000643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 1900/5000 [7:00:59<11:18:26, 13.13s/step, loss=0.1351, lr=0.000633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900/5000 | Loss: 0.1351 | LR: 0.000633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 1950/5000 [7:11:58<11:09:41, 13.17s/step, loss=0.1273, lr=0.000623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1950/5000 | Loss: 0.1273 | LR: 0.000623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2000/5000 [7:22:57<10:47:18, 12.95s/step, loss=0.1172, lr=0.000612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/5000 | Loss: 0.1172 | LR: 0.000612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 2050/5000 [7:33:51<10:36:15, 12.94s/step, loss=0.1141, lr=0.000602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2050/5000 | Loss: 0.1141 | LR: 0.000602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 2100/5000 [7:44:48<10:31:52, 13.07s/step, loss=0.1160, lr=0.000592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100/5000 | Loss: 0.1160 | LR: 0.000592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 2150/5000 [7:55:44<10:20:02, 13.05s/step, loss=0.1083, lr=0.000582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2150/5000 | Loss: 0.1083 | LR: 0.000582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 2200/5000 [8:06:39<10:10:03, 13.07s/step, loss=0.1039, lr=0.000572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200/5000 | Loss: 0.1039 | LR: 0.000572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 2250/5000 [8:17:39<10:04:09, 13.18s/step, loss=0.1040, lr=0.000561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2250/5000 | Loss: 0.1040 | LR: 0.000561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 2300/5000 [8:28:20<9:39:08, 12.87s/step, loss=0.1143, lr=0.000551] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2300/5000 | Loss: 0.1143 | LR: 0.000551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 2350/5000 [8:39:12<9:39:58, 13.13s/step, loss=0.1063, lr=0.000541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2350/5000 | Loss: 0.1063 | LR: 0.000541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 2400/5000 [8:50:07<9:21:11, 12.95s/step, loss=0.0919, lr=0.000531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2400/5000 | Loss: 0.0919 | LR: 0.000531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 2450/5000 [9:01:04<9:15:59, 13.08s/step, loss=0.0985, lr=0.000521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2450/5000 | Loss: 0.0985 | LR: 0.000521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2500/5000 [9:11:59<9:00:24, 12.97s/step, loss=0.1093, lr=0.000510]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500/5000 | Loss: 0.1093 | LR: 0.000510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 2550/5000 [9:22:55<8:52:17, 13.04s/step, loss=0.1082, lr=0.000500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2550/5000 | Loss: 0.1082 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 2600/5000 [9:33:50<8:34:44, 12.87s/step, loss=0.0873, lr=0.000490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2600/5000 | Loss: 0.0873 | LR: 0.000490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 2650/5000 [9:44:50<8:35:52, 13.17s/step, loss=0.0911, lr=0.000480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2650/5000 | Loss: 0.0911 | LR: 0.000480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2700/5000 [9:55:47<8:21:14, 13.08s/step, loss=0.0852, lr=0.000470]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2700/5000 | Loss: 0.0852 | LR: 0.000470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 2750/5000 [10:06:45<8:14:19, 13.18s/step, loss=0.0993, lr=0.000459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2750/5000 | Loss: 0.0993 | LR: 0.000459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 2800/5000 [10:17:41<8:04:36, 13.22s/step, loss=0.1024, lr=0.000449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2800/5000 | Loss: 0.1024 | LR: 0.000449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 2850/5000 [10:28:39<7:48:51, 13.08s/step, loss=0.0906, lr=0.000439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2850/5000 | Loss: 0.0906 | LR: 0.000439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 2900/5000 [10:39:45<7:47:41, 13.36s/step, loss=0.0987, lr=0.000429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2900/5000 | Loss: 0.0987 | LR: 0.000429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 2950/5000 [10:50:43<7:19:04, 12.85s/step, loss=0.0980, lr=0.000419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2950/5000 | Loss: 0.0980 | LR: 0.000419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3000/5000 [11:01:38<7:18:30, 13.16s/step, loss=0.0878, lr=0.000408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000/5000 | Loss: 0.0878 | LR: 0.000408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 3050/5000 [11:12:32<7:07:01, 13.14s/step, loss=0.0895, lr=0.000398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3050/5000 | Loss: 0.0895 | LR: 0.000398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3100/5000 [11:23:26<6:56:31, 13.15s/step, loss=0.0930, lr=0.000388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3100/5000 | Loss: 0.0930 | LR: 0.000388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 3150/5000 [11:34:15<6:43:42, 13.09s/step, loss=0.0933, lr=0.000378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3150/5000 | Loss: 0.0933 | LR: 0.000378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 3200/5000 [11:45:10<6:28:19, 12.94s/step, loss=0.0985, lr=0.000368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3200/5000 | Loss: 0.0985 | LR: 0.000368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 3250/5000 [11:56:03<6:21:39, 13.09s/step, loss=0.0858, lr=0.000357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3250/5000 | Loss: 0.0858 | LR: 0.000357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 3300/5000 [12:06:58<6:13:32, 13.18s/step, loss=0.0855, lr=0.000347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3300/5000 | Loss: 0.0855 | LR: 0.000347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 3350/5000 [12:17:55<6:02:54, 13.20s/step, loss=0.0830, lr=0.000337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3350/5000 | Loss: 0.0830 | LR: 0.000337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 3400/5000 [12:28:51<5:47:22, 13.03s/step, loss=0.0876, lr=0.000327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3400/5000 | Loss: 0.0876 | LR: 0.000327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 3450/5000 [12:39:45<5:38:09, 13.09s/step, loss=0.0817, lr=0.000317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3450/5000 | Loss: 0.0817 | LR: 0.000317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 3500/5000 [12:50:37<5:27:35, 13.10s/step, loss=0.0807, lr=0.000306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500/5000 | Loss: 0.0807 | LR: 0.000306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 3550/5000 [13:01:32<5:15:52, 13.07s/step, loss=0.0830, lr=0.000296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3550/5000 | Loss: 0.0830 | LR: 0.000296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 3600/5000 [13:12:24<5:06:40, 13.14s/step, loss=0.0776, lr=0.000286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3600/5000 | Loss: 0.0776 | LR: 0.000286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 3650/5000 [13:23:18<4:53:07, 13.03s/step, loss=0.0812, lr=0.000276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3650/5000 | Loss: 0.0812 | LR: 0.000276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 3700/5000 [13:34:11<4:41:27, 12.99s/step, loss=0.0834, lr=0.000266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3700/5000 | Loss: 0.0834 | LR: 0.000266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 3750/5000 [13:45:05<4:34:40, 13.18s/step, loss=0.0780, lr=0.000255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3750/5000 | Loss: 0.0780 | LR: 0.000255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 3800/5000 [13:56:00<4:22:06, 13.11s/step, loss=0.0782, lr=0.000245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3800/5000 | Loss: 0.0782 | LR: 0.000245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 3850/5000 [14:06:55<4:11:57, 13.15s/step, loss=0.0729, lr=0.000235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3850/5000 | Loss: 0.0729 | LR: 0.000235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 3900/5000 [14:17:47<3:59:38, 13.07s/step, loss=0.0829, lr=0.000225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3900/5000 | Loss: 0.0829 | LR: 0.000225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 3950/5000 [14:28:42<3:48:33, 13.06s/step, loss=0.0764, lr=0.000214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3950/5000 | Loss: 0.0764 | LR: 0.000214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4000/5000 [14:39:36<3:39:00, 13.14s/step, loss=0.0759, lr=0.000204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/5000 | Loss: 0.0759 | LR: 0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 4050/5000 [14:50:28<3:26:47, 13.06s/step, loss=0.0764, lr=0.000194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4050/5000 | Loss: 0.0764 | LR: 0.000194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 4100/5000 [15:01:15<3:12:37, 12.84s/step, loss=0.0717, lr=0.000184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4100/5000 | Loss: 0.0717 | LR: 0.000184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 4150/5000 [15:12:02<3:05:20, 13.08s/step, loss=0.0738, lr=0.000174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4150/5000 | Loss: 0.0738 | LR: 0.000174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 4200/5000 [15:22:54<2:52:16, 12.92s/step, loss=0.0733, lr=0.000163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4200/5000 | Loss: 0.0733 | LR: 0.000163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4250/5000 [15:33:43<2:41:22, 12.91s/step, loss=0.0689, lr=0.000153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4250/5000 | Loss: 0.0689 | LR: 0.000153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 4300/5000 [15:44:28<2:29:51, 12.84s/step, loss=0.0738, lr=0.000143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4300/5000 | Loss: 0.0738 | LR: 0.000143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 4350/5000 [15:55:17<2:19:52, 12.91s/step, loss=0.0689, lr=0.000133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4350/5000 | Loss: 0.0689 | LR: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 4400/5000 [16:06:08<2:10:09, 13.02s/step, loss=0.0712, lr=0.000123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4400/5000 | Loss: 0.0712 | LR: 0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 4450/5000 [16:16:56<1:59:22, 13.02s/step, loss=0.0703, lr=0.000112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4450/5000 | Loss: 0.0703 | LR: 0.000112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 4500/5000 [16:27:46<1:48:03, 12.97s/step, loss=0.0707, lr=0.000102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500/5000 | Loss: 0.0707 | LR: 0.000102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 4550/5000 [16:38:32<1:37:03, 12.94s/step, loss=0.0675, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4550/5000 | Loss: 0.0675 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 4600/5000 [16:49:21<1:25:48, 12.87s/step, loss=0.0728, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4600/5000 | Loss: 0.0728 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4650/5000 [17:00:07<1:14:54, 12.84s/step, loss=0.0699, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4650/5000 | Loss: 0.0699 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 4700/5000 [17:10:52<1:04:32, 12.91s/step, loss=0.0654, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4700/5000 | Loss: 0.0654 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 4750/5000 [17:21:43<54:15, 13.02s/step, loss=0.0671, lr=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4750/5000 | Loss: 0.0671 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 4800/5000 [17:32:34<43:41, 13.11s/step, loss=0.0667, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4800/5000 | Loss: 0.0667 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 4850/5000 [17:43:23<32:23, 12.96s/step, loss=0.0637, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4850/5000 | Loss: 0.0637 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 4900/5000 [17:54:12<21:32, 12.93s/step, loss=0.0663, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4900/5000 | Loss: 0.0663 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 4950/5000 [18:05:01<10:48, 12.97s/step, loss=0.0634, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4950/5000 | Loss: 0.0634 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [18:15:51<00:00, 13.15s/step, loss=0.0671, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000/5000 | Loss: 0.0671 | LR: 0.000100\n",
      "\n",
      "✅ Reached 5000 steps - stopping training!\n",
      "\n",
      "======================================================================\n",
      "PHASE 1 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "✅ Checkpoint saved to: checkpoint_5000.pt\n",
      "✅ Trained for exactly 5000 steps\n",
      "✅ Final loss: 0.0671\n",
      "\n",
      "You can now run PHASE 2 to continue training for 500 more steps!\n",
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Import DeepSeekV3 from model.py\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, block_size=1024):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx:idx + self.block_size]\n",
    "        target_ids = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: TRAIN FOR 5000 STEPS (Plain PyTorch)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: TRAINING FOR 5000 STEPS (Plain PyTorch)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODEL = False  # Set to False to use full 135M model\n",
    "\n",
    "if USE_SMALL_MODEL:\n",
    "    print(\"\\n[Using SMALL model]\")\n",
    "    config = DeepSeek-V3Config(\n",
    "        vocab_size=50257,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1024,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=6,\n",
    "        num_key_value_heads=2,\n",
    "        max_position_embeddings=1024,\n",
    "        rms_norm_eps=1e-5,\n",
    "        rope_theta=10000,\n",
    "        tie_word_embeddings=True,\n",
    "        attention_dropout=0.0,\n",
    "    )\n",
    "    block_size = 512\n",
    "    batch_size = 4\n",
    "    max_lr = 3e-4\n",
    "    accumulate_grad_batches = 4\n",
    "else:\n",
    "    print(\"\\n[Using FULL DeepSeek-V3-135M architecture]\")\n",
    "    config = DeepSeekV3Config(\n",
    "        vocab_size=50257,\n",
    "        hidden_size=576,\n",
    "        num_hidden_layers=30,\n",
    "        num_attention_heads=9,\n",
    "        kv_lora_rank=512,\n",
    "        moe_intermediate_size=160,\n",
    "        n_shared_experts=1,\n",
    "        n_routed_experts=8,\n",
    "        num_experts_per_tok=2,\n",
    "        max_position_embeddings=2048,\n",
    "        rms_norm_eps=1e-5,\n",
    "        rope_theta=10000.0,\n",
    "        tie_word_embeddings=True,\n",
    "        attention_dropout=0.0,\n",
    "    )\n",
    "    block_size = 1024\n",
    "    batch_size = 2\n",
    "    max_lr = 1e-3\n",
    "    accumulate_grad_batches = 8\n",
    "\n",
    "# Training parameters\n",
    "warmup_steps = 100\n",
    "max_steps = 5000  # Will stop at EXACTLY 5000 steps\n",
    "log_interval = 50  # Print every 50 steps to avoid output overflow\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"\\nUsing device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"\\nUsing device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TextDataset(\"input-1.txt\", tokenizer, block_size)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DeepSeekV3ForCausalLM(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Architecture: DeepSeek-V3 (MLA + MoE)\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"  - Hidden size: {config.hidden_size}\")\n",
    "print(f\"  - Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  - Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  - KV LoRA Rank: {config.kv_lora_rank}\")\n",
    "print(f\"  - MoE Experts: 1 Shared + {config.n_routed_experts} Routed\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration (Phase 1):\")\n",
    "print(f\"  - Max steps: {max_steps}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {accumulate_grad_batches}\")\n",
    "print(f\"  - Effective batch size: {batch_size * accumulate_grad_batches}\")\n",
    "print(f\"  - Warmup steps: {warmup_steps}\")\n",
    "print(f\"  - Max learning rate: {max_lr}\")\n",
    "print(f\"  - Logging interval: Every {log_interval} steps\")\n",
    "\n",
    "# Optimizer - separate decay parameters\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'bias' in name or 'norm' in name or 'embed' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.01},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=max_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        return max_lr * max(0.1, 1.0 - progress)\n",
    "\n",
    "# Clear cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ==========================================\n",
    "# INITIAL LOSS CHECK\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INITIAL LOSS CHECK (before training)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample batch\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    input_ids, target_ids = sample_batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    initial_loss = outputs['loss'].item()\n",
    "    \n",
    "    print(f\"\\nInitial loss (random weights): {initial_loss:.4f}\")\n",
    "    print(f\"Expected loss for random model: ~{math.log(config.vocab_size):.2f}\")\n",
    "    \n",
    "    if initial_loss > 50:\n",
    "        print(f\"\\n⚠️  WARNING: Loss is unusually high ({initial_loss:.4f})!\")\n",
    "        print(\"This suggests a potential issue with loss calculation.\")\n",
    "    elif 8 < initial_loss < 15:\n",
    "        print(f\"\\n✅ Loss is in expected range for random initialization!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Loss is {initial_loss:.4f}, which is outside typical range (8-15)\")\n",
    "\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "model.train()\n",
    "\n",
    "# ==========================================\n",
    "# TRAINING LOOP\n",
    "# ==========================================\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING PHASE 1 TRAINING\")\n",
    "print(f\"Will stop at EXACTLY {max_steps} steps\")\n",
    "print(f\"Logging every {log_interval} steps\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Training loop\n",
    "update_step = 0  # Actual optimizer update steps\n",
    "batch_idx = 0    # Batch counter for gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Create infinite dataloader\n",
    "def cycle(dataloader):\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            yield batch\n",
    "\n",
    "data_iter = cycle(dataloader)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=max_steps, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "while update_step < max_steps:\n",
    "    # Get batch\n",
    "    input_ids, target_ids = next(data_iter)\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # Scale loss for gradient accumulation\n",
    "    loss = loss / accumulate_grad_batches\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate the UNSCALED loss for logging (multiply back)\n",
    "    accumulated_loss += loss.item() * accumulate_grad_batches\n",
    "    batch_idx += 1\n",
    "\n",
    "    # Update weights after accumulation\n",
    "    if batch_idx % accumulate_grad_batches == 0:\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(update_step, warmup_steps, max_steps, max_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate average loss over accumulated batches\n",
    "        avg_loss = accumulated_loss / accumulate_grad_batches\n",
    "        \n",
    "        # Update progress bar every step\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "        \n",
    "        # Print only at log_interval to avoid output overflow\n",
    "        if (update_step + 1) % log_interval == 0 or update_step == 0:\n",
    "            print(f\"Step {update_step + 1}/{max_steps} | Loss: {avg_loss:.4f} | LR: {lr:.6f}\")\n",
    "        \n",
    "        accumulated_loss = 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        update_step += 1\n",
    "        batch_idx = 0  # Reset batch counter\n",
    "\n",
    "        # EXACT STOP at max_steps\n",
    "        if update_step >= max_steps:\n",
    "            print(f\"\\n✅ Reached {max_steps} steps - stopping training!\")\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1 COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = \"checkpoint_5000.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'total_params': total_params,\n",
    "    'global_step': update_step,\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"\\n✅ Checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"✅ Trained for exactly {update_step} steps\")\n",
    "print(f\"✅ Final loss: {avg_loss:.4f}\")\n",
    "print(\"\\nYou can now run PHASE 2 to continue training for 500 more steps!\")\n",
    "\n",
    "# Cleanup\n",
    "del model, optimizer\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6nqsuqahkp",
   "metadata": {},
   "source": [
    "# PHASE 2: Load checkpoint and train for 500 more steps\n",
    "\n",
    "Run this cell after Phase 1 completes. It will:\n",
    "- Load the checkpoint from `checkpoint_5000.pt`\n",
    "- Resume training from step 5000\n",
    "- Train for 500 more steps (total: 5500 steps)\n",
    "- Save final checkpoint to `checkpoint_5500.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d7sr7a1ep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: LOADING CHECKPOINT AND TRAINING 500 MORE STEPS\n",
      "======================================================================\n",
      "\n",
      "📂 Loading checkpoint from: checkpoint_5000.pt\n",
      "✅ Checkpoint loaded successfully!\n",
      "   Previous training: 5000 steps\n",
      "\n",
      "Phase 2 Training Plan:\n",
      "  - Starting from step: 5000\n",
      "  - Training for: 500 more steps\n",
      "  - Final step will be: 5500\n",
      "  - Logging interval: Every 50 steps\n",
      "\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "\n",
      "✅ Model weights loaded from checkpoint\n",
      "   Total parameters: 144,152,448\n",
      "✅ Optimizer state restored!\n",
      "\n",
      "======================================================================\n",
      "STARTING PHASE 2 TRAINING\n",
      "Resuming from step 5000, training to step 5500\n",
      "Will stop at EXACTLY 5500 steps\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 5001/5500 [00:16<2:16:35, 16.42s/step, loss=0.0644, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5001/5500 | Loss: 0.0644 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 5050/5500 [10:42<1:34:41, 12.63s/step, loss=0.0696, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5050/5500 | Loss: 0.0696 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 5100/5500 [21:10<1:24:30, 12.68s/step, loss=0.0629, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5100/5500 | Loss: 0.0629 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 5150/5500 [31:44<1:14:48, 12.82s/step, loss=0.0692, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5150/5500 | Loss: 0.0692 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 5200/5500 [42:23<1:04:14, 12.85s/step, loss=0.0626, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5200/5500 | Loss: 0.0626 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 5250/5500 [53:04<53:36, 12.87s/step, loss=0.0603, lr=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5250/5500 | Loss: 0.0603 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▋| 5300/5500 [1:03:49<43:06, 12.93s/step, loss=0.0675, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5300/5500 | Loss: 0.0675 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 5350/5500 [1:14:42<32:45, 13.10s/step, loss=0.0602, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5350/5500 | Loss: 0.0602 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 5400/5500 [1:25:17<21:10, 12.70s/step, loss=0.0606, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5400/5500 | Loss: 0.0606 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 5450/5500 [1:36:03<10:49, 12.99s/step, loss=0.0652, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5450/5500 | Loss: 0.0652 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5500/5500 [1:46:57<00:00, 12.83s/step, loss=0.0617, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5500/5500 | Loss: 0.0617 | LR: 0.000100\n",
      "\n",
      "✅ Reached 5500 steps - stopping training!\n",
      "\n",
      "======================================================================\n",
      "PHASE 2 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "✅ Final checkpoint saved to: checkpoint_5500.pt\n",
      "✅ Total training steps: 5500\n",
      "✅ Final loss: 0.0617\n",
      "\n",
      "🎉 Training complete! Model trained for 5000 + 500 = 5500 steps\n",
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Import DeepSeekV3 from model.py\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, block_size=1024):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx:idx + self.block_size]\n",
    "        target_ids = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: LOAD CHECKPOINT AND TRAIN 500 MORE STEPS (Plain PyTorch)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: LOADING CHECKPOINT AND TRAINING 500 MORE STEPS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = \"checkpoint_5000.pt\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"\\n❌ ERROR: Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please run PHASE 1 first to create the checkpoint!\")\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"\\n📂 Loading checkpoint from: {checkpoint_path}\")\n",
    "# Note: weights_only=False is safe here because we created this checkpoint ourselves\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "previous_step = checkpoint['global_step']\n",
    "\n",
    "print(f\"✅ Checkpoint loaded successfully!\")\n",
    "print(f\"   Previous training: {previous_step} steps\")\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODEL = False  # Should match Phase 1\n",
    "\n",
    "if USE_SMALL_MODEL:\n",
    "    block_size = 512\n",
    "    batch_size = 4\n",
    "    max_lr = 3e-4\n",
    "    accumulate_grad_batches = 4\n",
    "else:\n",
    "    block_size = 1024\n",
    "    batch_size = 2\n",
    "    max_lr = 1e-3\n",
    "    accumulate_grad_batches = 8\n",
    "\n",
    "# Training parameters for PHASE 2\n",
    "warmup_steps = 100  # Already completed in Phase 1\n",
    "additional_steps = 500  # Train for 500 MORE steps\n",
    "new_max_steps = previous_step + additional_steps  # Total: 5500 steps\n",
    "log_interval = 50  # Print every 50 steps to avoid output overflow\n",
    "\n",
    "print(f\"\\nPhase 2 Training Plan:\")\n",
    "print(f\"  - Starting from step: {previous_step}\")\n",
    "print(f\"  - Training for: {additional_steps} more steps\")\n",
    "print(f\"  - Final step will be: {new_max_steps}\")\n",
    "print(f\"  - Logging interval: Every {log_interval} steps\")\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"\\nUsing device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"\\nUsing device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TextDataset(\"input-1.txt\", tokenizer, block_size)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DeepSeekV3ForCausalLM(config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n✅ Model weights loaded from checkpoint\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "\n",
    "# Optimizer - separate decay parameters\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'bias' in name or 'norm' in name or 'embed' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.01},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=max_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# Load optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"✅ Optimizer state restored!\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        return max_lr * max(0.1, 1.0 - progress)\n",
    "\n",
    "# Clear cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING PHASE 2 TRAINING\")\n",
    "print(f\"Resuming from step {previous_step}, training to step {new_max_steps}\")\n",
    "print(f\"Will stop at EXACTLY {new_max_steps} steps\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "update_step = previous_step  # Start from where we left off\n",
    "batch_idx = 0  # Batch counter for gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Create infinite dataloader\n",
    "def cycle(dataloader):\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            yield batch\n",
    "\n",
    "data_iter = cycle(dataloader)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=new_max_steps, initial=previous_step, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "while update_step < new_max_steps:\n",
    "    # Get batch\n",
    "    input_ids, target_ids = next(data_iter)\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # Scale loss for gradient accumulation\n",
    "    loss = loss / accumulate_grad_batches\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate the UNSCALED loss for logging (multiply back)\n",
    "    accumulated_loss += loss.item() * accumulate_grad_batches\n",
    "    batch_idx += 1\n",
    "\n",
    "    # Update weights after accumulation\n",
    "    if batch_idx % accumulate_grad_batches == 0:\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(update_step, warmup_steps, new_max_steps, max_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate average loss over accumulated batches\n",
    "        avg_loss = accumulated_loss / accumulate_grad_batches\n",
    "        \n",
    "        # Update progress bar every step\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "        \n",
    "        # Print only at log_interval to avoid output overflow\n",
    "        if (update_step + 1) % log_interval == 0 or update_step == previous_step:\n",
    "            print(f\"Step {update_step + 1}/{new_max_steps} | Loss: {avg_loss:.4f} | LR: {lr:.6f}\")\n",
    "        \n",
    "        accumulated_loss = 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        update_step += 1\n",
    "        batch_idx = 0  # Reset batch counter\n",
    "\n",
    "        # EXACT STOP at new_max_steps\n",
    "        if update_step >= new_max_steps:\n",
    "            print(f\"\\n✅ Reached {new_max_steps} steps - stopping training!\")\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2 COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = \"checkpoint_5500.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'total_params': total_params,\n",
    "    'global_step': update_step,\n",
    "}, final_checkpoint_path)\n",
    "\n",
    "print(f\"\\n✅ Final checkpoint saved to: {final_checkpoint_path}\")\n",
    "print(f\"✅ Total training steps: {update_step}\")\n",
    "print(f\"✅ Final loss: {avg_loss:.4f}\")\n",
    "print(f\"\\n🎉 Training complete! Model trained for {previous_step} + {additional_steps} = {update_step} steps\")\n",
    "\n",
    "# Cleanup\n",
    "del model, optimizer\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
