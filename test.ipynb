{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w8wmqv8ntq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "⚠️  ERROR: No trained model found!\n",
      "======================================================================\n",
      "\n",
      "The checkpoint file 'checkpoint_5500.pt' does not exist.\n",
      "\n",
      "Please run the training cell FIRST to create the model.\n",
      "\n",
      "The training cell will:\n",
      "  1. Create and train DeepSeek-V3 from scratch\n",
      "  2. Save the trained model to 'checkpoint_5500.pt'\n",
      "  3. Then you can run this generation cell\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# TEXT GENERATION - Run this AFTER training completes\n",
    "import torch\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = \"checkpoint_5500.pt\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(\"=\" * 70)\n",
    "    print(\"⚠️  ERROR: No trained model found!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nThe checkpoint file '{checkpoint_path}' does not exist.\")\n",
    "    print(\"\\nPlease run the training cell FIRST to create the model.\")\n",
    "    print(\"\\nThe training cell will:\")\n",
    "    print(\"  1. Create and train DeepSeek-V3 from scratch\")\n",
    "    print(\"  2. Save the trained model to 'checkpoint_5500.pt'\")\n",
    "    print(\"  3. Then you can run this generation cell\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "else:\n",
    "    # Load the trained model\n",
    "    print(\"Loading trained model...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    config = checkpoint['config']\n",
    "    model = DeepSeekV3ForCausalLM(config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\"Using device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\n✅ Model loaded successfully!\")\n",
    "    print(f\"Parameters: {checkpoint['total_params']:,}\")\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Generate text\n",
    "    def generate_text(prompt_text, max_new_tokens=100, temperature=0.8, top_k=50):\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompt_text)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt_text}'\")\n",
    "        print(f\"Generating {max_new_tokens} tokens...\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 70)\n",
    "        return generated_text\n",
    "\n",
    "    # Example generations\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEXT GENERATION EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Example 1\n",
    "    generate_text(\"Once upon a time\", max_new_tokens=50, temperature=0.8)\n",
    "\n",
    "    # Example 2\n",
    "    generate_text(\"The meaning of life is\", max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "    # Example 3 - Custom prompt (uncomment to use)\n",
    "    # generate_text(\"Your custom prompt here\", max_new_tokens=100, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4ck8qupl",
   "metadata": {},
   "source": [
    "# PHASE 1: Train for 5000 steps and save checkpoint\n",
    "\n",
    "Run this cell first. It will:\n",
    "- Train your model for exactly 5000 steps\n",
    "- Automatically stop after 5000 steps\n",
    "- Save a checkpoint to `checkpoint_5000.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pwb574lt2yt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: TRAINING FOR 5000 STEPS (Plain PyTorch)\n",
      "======================================================================\n",
      "\n",
      "[Using FULL DeepSeek-V3-135M architecture]\n",
      "\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "\n",
      "Model Configuration:\n",
      "  - Architecture: DeepSeek-V3 (MLA + MoE)\n",
      "  - Vocabulary size: 50,257\n",
      "  - Hidden size: 576\n",
      "  - Layers: 30\n",
      "  - Attention heads: 9\n",
      "  - KV LoRA Rank: 512\n",
      "  - MoE Experts: 1 Shared + 8 Routed\n",
      "  - Total parameters: 174,012,288\n",
      "\n",
      "Training Configuration (Phase 1):\n",
      "  - Max steps: 5000\n",
      "  - Batch size: 2\n",
      "  - Gradient accumulation: 8\n",
      "  - Effective batch size: 16\n",
      "  - Warmup steps: 100\n",
      "  - Max learning rate: 0.001\n",
      "  - Logging interval: Every 50 steps\n",
      "\n",
      "======================================================================\n",
      "INITIAL LOSS CHECK (before training)\n",
      "======================================================================\n",
      "\n",
      "Initial loss (random weights): 10.9710\n",
      "Expected loss for random model: ~10.82\n",
      "\n",
      "✅ Loss is in expected range for random initialization!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STARTING PHASE 1 TRAINING\n",
      "Will stop at EXACTLY 5000 steps\n",
      "Logging every 50 steps\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/5000 [00:35<48:47:38, 35.14s/step, loss=11.1523, lr=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/5000 | Loss: 11.1523 | LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50/5000 [12:14<17:04:33, 12.42s/step, loss=6.3426, lr=0.000490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50/5000 | Loss: 6.3426 | LR: 0.000490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 100/5000 [23:09<20:32:24, 15.09s/step, loss=6.1240, lr=0.000990]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/5000 | Loss: 6.1240 | LR: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 150/5000 [36:05<21:20:46, 15.84s/step, loss=5.4385, lr=0.000990]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150/5000 | Loss: 5.4385 | LR: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 200/5000 [49:15<20:21:59, 15.27s/step, loss=5.0636, lr=0.000980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200/5000 | Loss: 5.0636 | LR: 0.000980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 250/5000 [1:01:50<19:38:42, 14.89s/step, loss=4.7123, lr=0.000970]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250/5000 | Loss: 4.7123 | LR: 0.000970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 300/5000 [1:14:20<19:53:27, 15.24s/step, loss=4.5043, lr=0.000959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300/5000 | Loss: 4.5043 | LR: 0.000959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 350/5000 [1:26:22<18:52:55, 14.62s/step, loss=4.1652, lr=0.000949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350/5000 | Loss: 4.1652 | LR: 0.000949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5000 [1:38:36<18:40:00, 14.61s/step, loss=3.6805, lr=0.000939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400/5000 | Loss: 3.6805 | LR: 0.000939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 450/5000 [1:50:50<18:22:51, 14.54s/step, loss=3.0098, lr=0.000929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450/5000 | Loss: 3.0098 | LR: 0.000929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 500/5000 [2:02:45<17:49:06, 14.25s/step, loss=2.6873, lr=0.000919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500/5000 | Loss: 2.6873 | LR: 0.000919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 550/5000 [2:14:36<17:45:03, 14.36s/step, loss=1.8318, lr=0.000908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 550/5000 | Loss: 1.8318 | LR: 0.000908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 600/5000 [2:26:30<17:30:40, 14.33s/step, loss=1.0757, lr=0.000898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600/5000 | Loss: 1.0757 | LR: 0.000898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 650/5000 [2:38:12<17:02:48, 14.11s/step, loss=0.5748, lr=0.000888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 650/5000 | Loss: 0.5748 | LR: 0.000888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 700/5000 [2:49:48<16:26:11, 13.76s/step, loss=0.4340, lr=0.000878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700/5000 | Loss: 0.4340 | LR: 0.000878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 750/5000 [3:01:28<16:40:00, 14.12s/step, loss=0.3486, lr=0.000868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 750/5000 | Loss: 0.3486 | LR: 0.000868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 800/5000 [3:13:06<16:16:44, 13.95s/step, loss=0.3401, lr=0.000857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800/5000 | Loss: 0.3401 | LR: 0.000857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 850/5000 [3:24:42<16:03:46, 13.93s/step, loss=0.2799, lr=0.000847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 850/5000 | Loss: 0.2799 | LR: 0.000847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 900/5000 [3:36:19<16:00:17, 14.05s/step, loss=0.2515, lr=0.000837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900/5000 | Loss: 0.2515 | LR: 0.000837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 950/5000 [3:47:52<15:30:52, 13.79s/step, loss=0.2300, lr=0.000827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 950/5000 | Loss: 0.2300 | LR: 0.000827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1000/5000 [3:59:28<15:36:16, 14.04s/step, loss=0.2030, lr=0.000817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/5000 | Loss: 0.2030 | LR: 0.000817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 1050/5000 [4:11:02<15:09:21, 13.81s/step, loss=0.2127, lr=0.000806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1050/5000 | Loss: 0.2127 | LR: 0.000806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 1100/5000 [4:22:37<15:04:40, 13.92s/step, loss=0.2036, lr=0.000796]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100/5000 | Loss: 0.2036 | LR: 0.000796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1150/5000 [4:34:14<14:55:26, 13.95s/step, loss=0.1756, lr=0.000786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1150/5000 | Loss: 0.1756 | LR: 0.000786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 1200/5000 [4:45:42<14:37:52, 13.86s/step, loss=0.1667, lr=0.000776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200/5000 | Loss: 0.1667 | LR: 0.000776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 1250/5000 [4:57:17<14:35:38, 14.01s/step, loss=0.1696, lr=0.000766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1250/5000 | Loss: 0.1696 | LR: 0.000766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 1300/5000 [5:08:49<13:44:03, 13.36s/step, loss=0.1456, lr=0.000755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300/5000 | Loss: 0.1456 | LR: 0.000755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 1350/5000 [5:20:23<14:01:15, 13.83s/step, loss=0.1908, lr=0.000745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1350/5000 | Loss: 0.1908 | LR: 0.000745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 1400/5000 [5:31:57<13:53:54, 13.90s/step, loss=0.1628, lr=0.000735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400/5000 | Loss: 0.1628 | LR: 0.000735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 1450/5000 [5:43:31<13:39:01, 13.84s/step, loss=0.1273, lr=0.000725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1450/5000 | Loss: 0.1273 | LR: 0.000725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 1500/5000 [5:55:05<13:28:58, 13.87s/step, loss=0.1493, lr=0.000714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500/5000 | Loss: 0.1493 | LR: 0.000714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1550/5000 [6:06:42<13:17:28, 13.87s/step, loss=0.1456, lr=0.000704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1550/5000 | Loss: 0.1456 | LR: 0.000704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 1600/5000 [6:18:15<13:05:27, 13.86s/step, loss=0.1315, lr=0.000694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600/5000 | Loss: 0.1315 | LR: 0.000694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1650/5000 [6:29:47<12:59:58, 13.97s/step, loss=0.1299, lr=0.000684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1650/5000 | Loss: 0.1299 | LR: 0.000684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 1700/5000 [6:41:21<12:43:35, 13.88s/step, loss=0.1255, lr=0.000674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700/5000 | Loss: 0.1255 | LR: 0.000674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 1750/5000 [6:52:53<12:31:02, 13.87s/step, loss=0.1234, lr=0.000663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1750/5000 | Loss: 0.1234 | LR: 0.000663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 1800/5000 [7:04:28<12:24:19, 13.96s/step, loss=0.1352, lr=0.000653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800/5000 | Loss: 0.1352 | LR: 0.000653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 1850/5000 [7:16:00<12:00:06, 13.72s/step, loss=0.1188, lr=0.000643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1850/5000 | Loss: 0.1188 | LR: 0.000643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 1900/5000 [7:27:33<12:02:38, 13.99s/step, loss=0.1174, lr=0.000633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900/5000 | Loss: 0.1174 | LR: 0.000633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 1950/5000 [7:39:05<11:46:19, 13.90s/step, loss=0.1084, lr=0.000623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1950/5000 | Loss: 0.1084 | LR: 0.000623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2000/5000 [7:50:39<11:35:52, 13.92s/step, loss=0.1069, lr=0.000612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/5000 | Loss: 0.1069 | LR: 0.000612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 2050/5000 [8:02:13<11:30:04, 14.04s/step, loss=0.1160, lr=0.000602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2050/5000 | Loss: 0.1160 | LR: 0.000602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 2100/5000 [8:13:43<11:08:16, 13.83s/step, loss=0.1054, lr=0.000592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100/5000 | Loss: 0.1054 | LR: 0.000592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 2150/5000 [8:25:16<10:58:34, 13.86s/step, loss=0.1106, lr=0.000582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2150/5000 | Loss: 0.1106 | LR: 0.000582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 2200/5000 [8:36:44<10:40:18, 13.72s/step, loss=0.1146, lr=0.000572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200/5000 | Loss: 0.1146 | LR: 0.000572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 2250/5000 [8:48:18<10:39:33, 13.95s/step, loss=0.1178, lr=0.000561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2250/5000 | Loss: 0.1178 | LR: 0.000561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 2300/5000 [8:59:45<10:18:09, 13.74s/step, loss=0.0973, lr=0.000551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2300/5000 | Loss: 0.0973 | LR: 0.000551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 2350/5000 [9:11:14<10:18:26, 14.00s/step, loss=0.0938, lr=0.000541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2350/5000 | Loss: 0.0938 | LR: 0.000541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 2400/5000 [9:22:42<9:58:37, 13.81s/step, loss=0.0923, lr=0.000531] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2400/5000 | Loss: 0.0923 | LR: 0.000531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 2450/5000 [9:34:14<10:02:02, 14.17s/step, loss=0.0881, lr=0.000521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2450/5000 | Loss: 0.0881 | LR: 0.000521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2500/5000 [9:45:40<9:26:49, 13.60s/step, loss=0.1014, lr=0.000510] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500/5000 | Loss: 0.1014 | LR: 0.000510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 2550/5000 [9:57:13<9:28:30, 13.92s/step, loss=0.0930, lr=0.000500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2550/5000 | Loss: 0.0930 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 2600/5000 [10:08:42<8:59:58, 13.50s/step, loss=0.1005, lr=0.000490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2600/5000 | Loss: 0.1005 | LR: 0.000490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 2650/5000 [10:20:11<8:57:37, 13.73s/step, loss=0.0884, lr=0.000480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2650/5000 | Loss: 0.0884 | LR: 0.000480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2700/5000 [10:31:38<8:48:10, 13.78s/step, loss=0.0974, lr=0.000470]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2700/5000 | Loss: 0.0974 | LR: 0.000470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 2750/5000 [10:43:12<8:42:44, 13.94s/step, loss=0.0888, lr=0.000459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2750/5000 | Loss: 0.0888 | LR: 0.000459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 2800/5000 [10:54:42<8:24:37, 13.76s/step, loss=0.0917, lr=0.000449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2800/5000 | Loss: 0.0917 | LR: 0.000449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 2850/5000 [11:06:10<8:16:12, 13.85s/step, loss=0.0857, lr=0.000439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2850/5000 | Loss: 0.0857 | LR: 0.000439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 2900/5000 [11:17:42<7:59:37, 13.70s/step, loss=0.0834, lr=0.000429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2900/5000 | Loss: 0.0834 | LR: 0.000429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 2950/5000 [11:29:07<7:43:04, 13.55s/step, loss=0.0826, lr=0.000419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2950/5000 | Loss: 0.0826 | LR: 0.000419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3000/5000 [11:40:35<7:37:53, 13.74s/step, loss=0.0852, lr=0.000408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000/5000 | Loss: 0.0852 | LR: 0.000408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 3050/5000 [11:52:00<7:28:20, 13.80s/step, loss=0.0812, lr=0.000398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3050/5000 | Loss: 0.0812 | LR: 0.000398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3100/5000 [12:03:37<7:29:55, 14.21s/step, loss=0.0813, lr=0.000388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3100/5000 | Loss: 0.0813 | LR: 0.000388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 3150/5000 [12:15:22<7:17:52, 14.20s/step, loss=0.0840, lr=0.000378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3150/5000 | Loss: 0.0840 | LR: 0.000378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 3200/5000 [12:26:55<6:50:28, 13.68s/step, loss=0.0869, lr=0.000368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3200/5000 | Loss: 0.0869 | LR: 0.000368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 3250/5000 [12:38:23<6:39:59, 13.71s/step, loss=0.0814, lr=0.000357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3250/5000 | Loss: 0.0814 | LR: 0.000357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 3300/5000 [12:49:53<6:29:29, 13.75s/step, loss=0.0800, lr=0.000347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3300/5000 | Loss: 0.0800 | LR: 0.000347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 3350/5000 [13:01:26<6:20:49, 13.85s/step, loss=0.0815, lr=0.000337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3350/5000 | Loss: 0.0815 | LR: 0.000337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 3400/5000 [13:13:01<6:14:39, 14.05s/step, loss=0.0845, lr=0.000327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3400/5000 | Loss: 0.0845 | LR: 0.000327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 3450/5000 [13:24:32<5:54:01, 13.70s/step, loss=0.0756, lr=0.000317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3450/5000 | Loss: 0.0756 | LR: 0.000317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 3500/5000 [13:36:00<5:40:43, 13.63s/step, loss=0.0782, lr=0.000306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500/5000 | Loss: 0.0782 | LR: 0.000306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 3550/5000 [13:47:29<5:34:33, 13.84s/step, loss=0.0769, lr=0.000296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3550/5000 | Loss: 0.0769 | LR: 0.000296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 3600/5000 [13:58:59<5:17:52, 13.62s/step, loss=0.0705, lr=0.000286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3600/5000 | Loss: 0.0705 | LR: 0.000286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 3650/5000 [14:10:26<5:07:52, 13.68s/step, loss=0.0808, lr=0.000276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3650/5000 | Loss: 0.0808 | LR: 0.000276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 3700/5000 [14:21:58<5:01:48, 13.93s/step, loss=0.0751, lr=0.000266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3700/5000 | Loss: 0.0751 | LR: 0.000266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 3750/5000 [14:33:29<4:49:25, 13.89s/step, loss=0.0714, lr=0.000255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3750/5000 | Loss: 0.0714 | LR: 0.000255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 3800/5000 [14:44:55<4:32:46, 13.64s/step, loss=0.0739, lr=0.000245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3800/5000 | Loss: 0.0739 | LR: 0.000245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 3850/5000 [14:56:21<4:22:54, 13.72s/step, loss=0.0735, lr=0.000235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3850/5000 | Loss: 0.0735 | LR: 0.000235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 3900/5000 [15:07:46<4:09:44, 13.62s/step, loss=0.0738, lr=0.000225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3900/5000 | Loss: 0.0738 | LR: 0.000225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 3950/5000 [15:19:06<3:59:58, 13.71s/step, loss=0.0792, lr=0.000214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3950/5000 | Loss: 0.0792 | LR: 0.000214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4000/5000 [15:30:31<3:44:54, 13.49s/step, loss=0.0765, lr=0.000204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/5000 | Loss: 0.0765 | LR: 0.000204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 4050/5000 [15:41:52<3:37:48, 13.76s/step, loss=0.0708, lr=0.000194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4050/5000 | Loss: 0.0708 | LR: 0.000194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 4100/5000 [15:53:16<3:26:40, 13.78s/step, loss=0.0710, lr=0.000184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4100/5000 | Loss: 0.0710 | LR: 0.000184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 4150/5000 [16:33:15<44:57:48, 190.43s/step, loss=0.0681, lr=0.000174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4150/5000 | Loss: 0.0681 | LR: 0.000174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 4200/5000 [17:01:07<3:02:45, 13.71s/step, loss=0.0731, lr=0.000163]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4200/5000 | Loss: 0.0731 | LR: 0.000163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4250/5000 [17:48:38<6:02:59, 29.04s/step, loss=0.0634, lr=0.000153]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4250/5000 | Loss: 0.0634 | LR: 0.000153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 4300/5000 [18:56:43<44:32:49, 229.10s/step, loss=0.0666, lr=0.000143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4300/5000 | Loss: 0.0666 | LR: 0.000143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 4350/5000 [19:50:04<2:25:43, 13.45s/step, loss=0.0662, lr=0.000133]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4350/5000 | Loss: 0.0662 | LR: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 4400/5000 [20:52:28<6:19:52, 37.99s/step, loss=0.0661, lr=0.000123]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4400/5000 | Loss: 0.0661 | LR: 0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 4450/5000 [21:46:06<2:10:31, 14.24s/step, loss=0.0660, lr=0.000112]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4450/5000 | Loss: 0.0660 | LR: 0.000112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 4500/5000 [21:57:44<1:55:41, 13.88s/step, loss=0.0597, lr=0.000102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500/5000 | Loss: 0.0597 | LR: 0.000102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 4550/5000 [22:09:16<1:43:19, 13.78s/step, loss=0.0649, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4550/5000 | Loss: 0.0649 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 4600/5000 [22:20:45<1:33:02, 13.96s/step, loss=0.0657, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4600/5000 | Loss: 0.0657 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4650/5000 [22:32:11<1:19:56, 13.71s/step, loss=0.0687, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4650/5000 | Loss: 0.0687 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 4700/5000 [22:43:39<1:08:18, 13.66s/step, loss=0.0636, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4700/5000 | Loss: 0.0636 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 4750/5000 [22:55:07<57:05, 13.70s/step, loss=0.0625, lr=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4750/5000 | Loss: 0.0625 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 4800/5000 [23:06:36<46:24, 13.92s/step, loss=0.0717, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4800/5000 | Loss: 0.0717 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 4850/5000 [23:18:03<34:13, 13.69s/step, loss=0.0584, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4850/5000 | Loss: 0.0584 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 4900/5000 [23:29:32<22:44, 13.64s/step, loss=0.0648, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4900/5000 | Loss: 0.0648 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 4950/5000 [23:41:06<11:32, 13.85s/step, loss=0.0621, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4950/5000 | Loss: 0.0621 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [23:52:37<00:00, 17.19s/step, loss=0.0652, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000/5000 | Loss: 0.0652 | LR: 0.000100\n",
      "\n",
      "✅ Reached 5000 steps - stopping training!\n",
      "\n",
      "======================================================================\n",
      "PHASE 1 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "✅ Checkpoint saved to: checkpoint_5000.pt\n",
      "✅ Trained for exactly 5000 steps\n",
      "✅ Final loss: 0.0652\n",
      "\n",
      "You can now run PHASE 2 to continue training for 500 more steps!\n",
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Import DeepSeekV3 from model.py\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, block_size=1024):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx:idx + self.block_size]\n",
    "        target_ids = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: TRAIN FOR 5000 STEPS (Plain PyTorch)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: TRAINING FOR 5000 STEPS (Plain PyTorch)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODEL = False  # Set to False to use full 135M model\n",
    "\n",
    "if USE_SMALL_MODEL:\n",
    "    print(\"\\n[Using SMALL model]\")\n",
    "    config = DeepSeek-V3Config(\n",
    "        vocab_size=50257,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1024,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=6,\n",
    "        num_key_value_heads=2,\n",
    "        max_position_embeddings=1024,\n",
    "        rms_norm_eps=1e-5,\n",
    "        rope_theta=10000,\n",
    "        tie_word_embeddings=True,\n",
    "        attention_dropout=0.0,\n",
    "    )\n",
    "    block_size = 512\n",
    "    batch_size = 4\n",
    "    max_lr = 3e-4\n",
    "    accumulate_grad_batches = 4\n",
    "else:\n",
    "    print(\"\\n[Using FULL DeepSeek-V3-135M architecture]\")\n",
    "    config = DeepSeekV3Config(\n",
    "        vocab_size=50257,\n",
    "        hidden_size=576,\n",
    "        num_hidden_layers=30,\n",
    "        num_attention_heads=9,\n",
    "        kv_lora_rank=512,\n",
    "        moe_intermediate_size=256,\n",
    "        n_shared_experts=1,\n",
    "        n_routed_experts=8,\n",
    "        num_experts_per_tok=2,\n",
    "        max_position_embeddings=2048,\n",
    "        rms_norm_eps=1e-5,\n",
    "        rope_theta=10000.0,\n",
    "        tie_word_embeddings=True,\n",
    "        attention_dropout=0.0,\n",
    "    )\n",
    "    block_size = 1024\n",
    "    batch_size = 2\n",
    "    max_lr = 1e-3\n",
    "    accumulate_grad_batches = 8\n",
    "\n",
    "# Training parameters\n",
    "warmup_steps = 100\n",
    "max_steps = 5000  # Will stop at EXACTLY 5000 steps\n",
    "log_interval = 50  # Print every 50 steps to avoid output overflow\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"\\nUsing device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"\\nUsing device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TextDataset(\"input-1.txt\", tokenizer, block_size)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DeepSeekV3ForCausalLM(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Architecture: DeepSeek-V3 (MLA + MoE)\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"  - Hidden size: {config.hidden_size}\")\n",
    "print(f\"  - Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  - Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  - KV LoRA Rank: {config.kv_lora_rank}\")\n",
    "print(f\"  - MoE Experts: 1 Shared + {config.n_routed_experts} Routed\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration (Phase 1):\")\n",
    "print(f\"  - Max steps: {max_steps}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {accumulate_grad_batches}\")\n",
    "print(f\"  - Effective batch size: {batch_size * accumulate_grad_batches}\")\n",
    "print(f\"  - Warmup steps: {warmup_steps}\")\n",
    "print(f\"  - Max learning rate: {max_lr}\")\n",
    "print(f\"  - Logging interval: Every {log_interval} steps\")\n",
    "\n",
    "# Optimizer - separate decay parameters\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'bias' in name or 'norm' in name or 'embed' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.01},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=max_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        return max_lr * max(0.1, 1.0 - progress)\n",
    "\n",
    "# Clear cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ==========================================\n",
    "# INITIAL LOSS CHECK\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INITIAL LOSS CHECK (before training)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample batch\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    input_ids, target_ids = sample_batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    initial_loss = outputs['loss'].item()\n",
    "    \n",
    "    print(f\"\\nInitial loss (random weights): {initial_loss:.4f}\")\n",
    "    print(f\"Expected loss for random model: ~{math.log(config.vocab_size):.2f}\")\n",
    "    \n",
    "    if initial_loss > 50:\n",
    "        print(f\"\\n⚠️  WARNING: Loss is unusually high ({initial_loss:.4f})!\")\n",
    "        print(\"This suggests a potential issue with loss calculation.\")\n",
    "    elif 8 < initial_loss < 15:\n",
    "        print(f\"\\n✅ Loss is in expected range for random initialization!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Loss is {initial_loss:.4f}, which is outside typical range (8-15)\")\n",
    "\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "model.train()\n",
    "\n",
    "# ==========================================\n",
    "# TRAINING LOOP\n",
    "# ==========================================\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING PHASE 1 TRAINING\")\n",
    "print(f\"Will stop at EXACTLY {max_steps} steps\")\n",
    "print(f\"Logging every {log_interval} steps\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Training loop\n",
    "update_step = 0  # Actual optimizer update steps\n",
    "batch_idx = 0    # Batch counter for gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Create infinite dataloader\n",
    "def cycle(dataloader):\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            yield batch\n",
    "\n",
    "data_iter = cycle(dataloader)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=max_steps, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "while update_step < max_steps:\n",
    "    # Get batch\n",
    "    input_ids, target_ids = next(data_iter)\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # Scale loss for gradient accumulation\n",
    "    loss = loss / accumulate_grad_batches\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate the UNSCALED loss for logging (multiply back)\n",
    "    accumulated_loss += loss.item() * accumulate_grad_batches\n",
    "    batch_idx += 1\n",
    "\n",
    "    # Update weights after accumulation\n",
    "    if batch_idx % accumulate_grad_batches == 0:\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(update_step, warmup_steps, max_steps, max_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate average loss over accumulated batches\n",
    "        avg_loss = accumulated_loss / accumulate_grad_batches\n",
    "        \n",
    "        # Update progress bar every step\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "        \n",
    "        # Print only at log_interval to avoid output overflow\n",
    "        if (update_step + 1) % log_interval == 0 or update_step == 0:\n",
    "            print(f\"Step {update_step + 1}/{max_steps} | Loss: {avg_loss:.4f} | LR: {lr:.6f}\")\n",
    "        \n",
    "        accumulated_loss = 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        update_step += 1\n",
    "        batch_idx = 0  # Reset batch counter\n",
    "\n",
    "        # EXACT STOP at max_steps\n",
    "        if update_step >= max_steps:\n",
    "            print(f\"\\n✅ Reached {max_steps} steps - stopping training!\")\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1 COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = \"checkpoint_5000.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'total_params': total_params,\n",
    "    'global_step': update_step,\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"\\n✅ Checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"✅ Trained for exactly {update_step} steps\")\n",
    "print(f\"✅ Final loss: {avg_loss:.4f}\")\n",
    "print(\"\\nYou can now run PHASE 2 to continue training for 500 more steps!\")\n",
    "\n",
    "# Cleanup\n",
    "del model, optimizer\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6nqsuqahkp",
   "metadata": {},
   "source": [
    "# PHASE 2: Load checkpoint and train for 500 more steps\n",
    "\n",
    "Run this cell after Phase 1 completes. It will:\n",
    "- Load the checkpoint from `checkpoint_5000.pt`\n",
    "- Resume training from step 5000\n",
    "- Train for 500 more steps (total: 5500 steps)\n",
    "- Save final checkpoint to `checkpoint_5500.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d7sr7a1ep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: LOADING CHECKPOINT AND TRAINING 500 MORE STEPS\n",
      "======================================================================\n",
      "\n",
      "📂 Loading checkpoint from: checkpoint_5000.pt\n",
      "✅ Checkpoint loaded successfully!\n",
      "   Previous training: 5000 steps\n",
      "\n",
      "Phase 2 Training Plan:\n",
      "  - Starting from step: 5000\n",
      "  - Training for: 500 more steps\n",
      "  - Final step will be: 5500\n",
      "  - Logging interval: Every 50 steps\n",
      "\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "\n",
      "✅ Model weights loaded from checkpoint\n",
      "   Total parameters: 174,012,288\n",
      "✅ Optimizer state restored!\n",
      "\n",
      "======================================================================\n",
      "STARTING PHASE 2 TRAINING\n",
      "Resuming from step 5000, training to step 5500\n",
      "Will stop at EXACTLY 5500 steps\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 5001/5500 [00:31<4:25:49, 31.96s/step, loss=0.0647, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5001/5500 | Loss: 0.0647 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 5050/5500 [12:28<1:50:14, 14.70s/step, loss=0.0632, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5050/5500 | Loss: 0.0632 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 5100/5500 [24:36<1:36:33, 14.48s/step, loss=0.0633, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5100/5500 | Loss: 0.0633 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 5150/5500 [36:38<1:24:48, 14.54s/step, loss=0.0597, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5150/5500 | Loss: 0.0597 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 5200/5500 [48:41<1:12:21, 14.47s/step, loss=0.0621, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5200/5500 | Loss: 0.0621 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 5250/5500 [1:00:43<1:00:06, 14.43s/step, loss=0.0579, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5250/5500 | Loss: 0.0579 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▋| 5300/5500 [1:12:45<48:30, 14.55s/step, loss=0.0632, lr=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5300/5500 | Loss: 0.0632 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 5350/5500 [1:24:46<35:48, 14.33s/step, loss=0.0616, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5350/5500 | Loss: 0.0616 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 5400/5500 [1:36:52<24:11, 14.52s/step, loss=0.0641, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5400/5500 | Loss: 0.0641 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 5450/5500 [1:48:56<12:05, 14.52s/step, loss=0.0574, lr=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5450/5500 | Loss: 0.0574 | LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5500/5500 [2:00:59<00:00, 14.52s/step, loss=0.0609, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5500/5500 | Loss: 0.0609 | LR: 0.000100\n",
      "\n",
      "✅ Reached 5500 steps - stopping training!\n",
      "\n",
      "======================================================================\n",
      "PHASE 2 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "✅ Final checkpoint saved to: checkpoint_5500.pt\n",
      "✅ Total training steps: 5500\n",
      "✅ Final loss: 0.0609\n",
      "\n",
      "🎉 Training complete! Model trained for 5000 + 500 = 5500 steps\n",
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Import DeepSeekV3 from model.py\n",
    "from model import DeepSeekV3Config, DeepSeekV3ForCausalLM\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, block_size=1024):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx:idx + self.block_size]\n",
    "        target_ids = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: LOAD CHECKPOINT AND TRAIN 500 MORE STEPS (Plain PyTorch)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: LOADING CHECKPOINT AND TRAINING 500 MORE STEPS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = \"checkpoint_5000.pt\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"\\n❌ ERROR: Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please run PHASE 1 first to create the checkpoint!\")\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"\\n📂 Loading checkpoint from: {checkpoint_path}\")\n",
    "# Note: weights_only=False is safe here because we created this checkpoint ourselves\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "config = checkpoint['config']\n",
    "previous_step = checkpoint['global_step']\n",
    "\n",
    "print(f\"✅ Checkpoint loaded successfully!\")\n",
    "print(f\"   Previous training: {previous_step} steps\")\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODEL = False  # Should match Phase 1\n",
    "\n",
    "if USE_SMALL_MODEL:\n",
    "    block_size = 512\n",
    "    batch_size = 4\n",
    "    max_lr = 3e-4\n",
    "    accumulate_grad_batches = 4\n",
    "else:\n",
    "    block_size = 1024\n",
    "    batch_size = 2\n",
    "    max_lr = 1e-3\n",
    "    accumulate_grad_batches = 8\n",
    "\n",
    "# Training parameters for PHASE 2\n",
    "warmup_steps = 100  # Already completed in Phase 1\n",
    "additional_steps = 500  # Train for 500 MORE steps\n",
    "new_max_steps = previous_step + additional_steps  # Total: 5500 steps\n",
    "log_interval = 50  # Print every 50 steps to avoid output overflow\n",
    "\n",
    "print(f\"\\nPhase 2 Training Plan:\")\n",
    "print(f\"  - Starting from step: {previous_step}\")\n",
    "print(f\"  - Training for: {additional_steps} more steps\")\n",
    "print(f\"  - Final step will be: {new_max_steps}\")\n",
    "print(f\"  - Logging interval: Every {log_interval} steps\")\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"\\nUsing device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"\\nUsing device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TextDataset(\"input-1.txt\", tokenizer, block_size)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DeepSeekV3ForCausalLM(config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n✅ Model weights loaded from checkpoint\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "\n",
    "# Optimizer - separate decay parameters\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'bias' in name or 'norm' in name or 'embed' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.01},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=max_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# Load optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"✅ Optimizer state restored!\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        return max_lr * max(0.1, 1.0 - progress)\n",
    "\n",
    "# Clear cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING PHASE 2 TRAINING\")\n",
    "print(f\"Resuming from step {previous_step}, training to step {new_max_steps}\")\n",
    "print(f\"Will stop at EXACTLY {new_max_steps} steps\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "update_step = previous_step  # Start from where we left off\n",
    "batch_idx = 0  # Batch counter for gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Create infinite dataloader\n",
    "def cycle(dataloader):\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            yield batch\n",
    "\n",
    "data_iter = cycle(dataloader)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=new_max_steps, initial=previous_step, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "while update_step < new_max_steps:\n",
    "    # Get batch\n",
    "    input_ids, target_ids = next(data_iter)\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    # Scale loss for gradient accumulation\n",
    "    loss = loss / accumulate_grad_batches\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate the UNSCALED loss for logging (multiply back)\n",
    "    accumulated_loss += loss.item() * accumulate_grad_batches\n",
    "    batch_idx += 1\n",
    "\n",
    "    # Update weights after accumulation\n",
    "    if batch_idx % accumulate_grad_batches == 0:\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(update_step, warmup_steps, new_max_steps, max_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate average loss over accumulated batches\n",
    "        avg_loss = accumulated_loss / accumulate_grad_batches\n",
    "        \n",
    "        # Update progress bar every step\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "        \n",
    "        # Print only at log_interval to avoid output overflow\n",
    "        if (update_step + 1) % log_interval == 0 or update_step == previous_step:\n",
    "            print(f\"Step {update_step + 1}/{new_max_steps} | Loss: {avg_loss:.4f} | LR: {lr:.6f}\")\n",
    "        \n",
    "        accumulated_loss = 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        update_step += 1\n",
    "        batch_idx = 0  # Reset batch counter\n",
    "\n",
    "        # EXACT STOP at new_max_steps\n",
    "        if update_step >= new_max_steps:\n",
    "            print(f\"\\n✅ Reached {new_max_steps} steps - stopping training!\")\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2 COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = \"checkpoint_5500.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'total_params': total_params,\n",
    "    'global_step': update_step,\n",
    "}, final_checkpoint_path)\n",
    "\n",
    "print(f\"\\n✅ Final checkpoint saved to: {final_checkpoint_path}\")\n",
    "print(f\"✅ Total training steps: {update_step}\")\n",
    "print(f\"✅ Final loss: {avg_loss:.4f}\")\n",
    "print(f\"\\n🎉 Training complete! Model trained for {previous_step} + {additional_steps} = {update_step} steps\")\n",
    "\n",
    "# Cleanup\n",
    "del model, optimizer\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
